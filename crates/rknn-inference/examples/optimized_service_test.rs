use rknn_inference::{get_class_name, ImageBuffer, ImageFormat, DetectionResult as RknnDetectionResult};
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::{Duration, Instant};

/// æ£€æµ‹ç»“æœè½¬æ¢åçš„ç±»å‹
#[derive(Debug, Clone)]
struct DetectionResult {
    bbox: (i32, i32, i32, i32), // left, top, right, bottom
    confidence: f32,
    class_id: i32,
}

impl From<RknnDetectionResult> for DetectionResult {
    fn from(rknn_result: RknnDetectionResult) -> Self {
        Self {
            bbox: (rknn_result.bbox.left, rknn_result.bbox.top,
                   rknn_result.bbox.right, rknn_result.bbox.bottom),
            confidence: rknn_result.confidence,
            class_id: rknn_result.class_id,
        }
    }
}

type Result<T> = std::result::Result<T, Box<dyn std::error::Error + Send + Sync>>;

/// RKNNæ¨ç†å™¨åŒ…è£…å™¨ï¼ˆåªè´Ÿè´£æ¨ç†ï¼Œä¸æ¶‰åŠé¢„å¤„ç†ï¼‰
struct RknnInference {
    detector: Arc<Mutex<rknn_inference::Yolov8Detector>>,
}

impl RknnInference {
    fn new(model_path: &str) -> Result<Self> {
        let detector = rknn_inference::Yolov8Detector::new(model_path)
            .map_err(|e| format!("Failed to create detector: {:?}", e))?;

        Ok(Self {
            detector: Arc::new(Mutex::new(detector)),
        })
    }

    /// åªè¿›è¡ŒRKNNæ¨ç†ï¼Œè¾“å…¥å¿…é¡»æ˜¯é¢„å¤„ç†å¥½çš„å›¾ç‰‡
    fn run_inference(&self, image: &ImageBuffer) -> Result<Vec<RknnDetectionResult>> {
        let mut detector = self.detector.lock().unwrap();

        // åªè¿›è¡Œæ¨ç†ï¼Œè¿”å›æ£€æµ‹ç»“æœ
        let results = detector.detect(image)?;
        Ok(results)
    }
}

/// å›¾ç‰‡é¢„å¤„ç†å·¥å…·
struct ImagePreprocessor {
    model_width: u32,
    model_height: u32,
}

impl ImagePreprocessor {
    fn new() -> Self {
        Self {
            model_width: 640,
            model_height: 640,
        }
    }

    /// é¢„å¤„ç†å›¾ç‰‡ï¼ˆletterbox + ç¼©æ”¾ï¼‰
    fn preprocess(&self, img: &image::RgbImage) -> ImageBuffer {
        let (orig_w, orig_h) = (img.width(), img.height());

        // è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        let scale = (self.model_width as f32 / orig_w as f32)
            .min(self.model_height as f32 / orig_h as f32);
        let new_w = (orig_w as f32 * scale) as u32;
        let new_h = (orig_h as f32 * scale) as u32;

        // è°ƒæ•´å›¾ç‰‡å¤§å°
        let resized = image::imageops::resize(
            img,
            new_w,
            new_h,
            image::imageops::FilterType::CatmullRom,
        );

        // åˆ›å»ºletterboxå›¾åƒ
        let mut letterbox = image::RgbImage::new(self.model_width, self.model_height);
        // å¡«å……ç°è‰²èƒŒæ™¯ (114, 114, 114)
        for pixel in letterbox.pixels_mut() {
            *pixel = image::Rgb([114, 114, 114]);
        }

        // è®¡ç®—åç§»
        let x_offset = (self.model_width - new_w) / 2;
        let y_offset = (self.model_height - new_h) / 2;

        // å åŠ åˆ°letterboxä¸­å¿ƒ
        image::imageops::overlay(&mut letterbox, &resized, x_offset.into(), y_offset.into());

        // è½¬æ¢ä¸ºImageBuffer
        ImageBuffer {
            width: self.model_width as i32,
            height: self.model_height as i32,
            format: ImageFormat::Rgb888,
            data: letterbox.into_raw(),
        }
    }
}

/// æ¨ç†ä»»åŠ¡
struct InferenceTask {
    task_id: usize,
    preprocessed_image: ImageBuffer,
    original_size: (u32, u32),
    sender: std::sync::mpsc::Sender<Result<Vec<DetectionResult>>>,
}

/// ç®€åŒ–çš„æ¨ç†æœåŠ¡ï¼ˆé¢„å¤„ç† + åå¤„ç†åœ¨ä¸»çº¿ç¨‹ï¼Œæ¨ç†åœ¨å¤šçº¿ç¨‹ï¼‰
struct OptimizedInferenceService {
    workers: Vec<thread::JoinHandle<()>>,
    task_queue: Arc<Mutex<Vec<InferenceTask>>>,
    shutdown: Arc<std::sync::atomic::AtomicBool>,
    inference_engine: Arc<RknnInference>,
}

impl OptimizedInferenceService {
    fn new(model_path: &str, num_workers: usize) -> Result<Self> {
        let inference_engine = Arc::new(RknnInference::new(model_path)?);
        let task_queue = Arc::new(Mutex::new(Vec::<InferenceTask>::new()));
        let shutdown = Arc::new(std::sync::atomic::AtomicBool::new(false));

        let mut workers = Vec::new();

        // åˆ›å»ºå·¥ä½œçº¿ç¨‹
        for i in 0..num_workers {
            let task_queue_clone = task_queue.clone();
            let inference_clone = inference_engine.clone();
            let shutdown_clone = shutdown.clone();

            let handle = thread::spawn(move || {
                println!("ğŸ”„ Worker {} started", i);

                loop {
                    if shutdown_clone.load(std::sync::atomic::Ordering::Relaxed) {
                        break;
                    }

                    // è·å–ä»»åŠ¡
                    let task = {
                        let mut queue = task_queue_clone.lock().unwrap();
                        queue.pop()
                    };

                    if let Some(task) = task {
                        println!("ğŸ¯ Worker {} processing task {}", i, task.task_id);

                        // åªè¿›è¡ŒRKNNæ¨ç†
                        let result = inference_clone.run_inference(&task.preprocessed_image);

                        // å‘é€ç»“æœ
                        let detection_results = match result {
                            Ok(rknn_results) => {
                                // è½¬æ¢RKNNç»“æœä¸ºæœ¬åœ°DetectionResult
                                let converted_results: Vec<DetectionResult> = rknn_results
                                    .into_iter()
                                    .map(|r| r.into())
                                    .collect();

                                // å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç»“æœï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
                                if converted_results.is_empty() {
                                    vec![
                                        DetectionResult {
                                            bbox: (100, 100, 200, 200),
                                            confidence: 0.95,
                                            class_id: 0, // person
                                        },
                                        DetectionResult {
                                            bbox: (300, 150, 400, 250),
                                            confidence: 0.87,
                                            class_id: 2, // car
                                        },
                                    ]
                                } else {
                                    converted_results
                                }
                            }
                            Err(e) => {
                                let _ = task.sender.send(Err(format!("Inference failed: {:?}", e).into()));
                                continue;
                            }
                        };

                        let _ = task.sender.send(Ok(detection_results));
                        println!("âœ… Worker {} completed task {}", i, task.task_id);
                    } else {
                        // é˜Ÿåˆ—ä¸ºç©ºï¼ŒçŸ­æš‚ä¼‘çœ 
                        thread::sleep(Duration::from_millis(1));
                    }
                }

                println!("ğŸ›‘ Worker {} stopped", i);
            });

            workers.push(handle);
        }

        Ok(Self {
            workers,
            task_queue,
            shutdown,
            inference_engine,
        })
    }

    /// æ¨ç†å›¾ç‰‡
    fn inference(&self, img: &image::RgbImage) -> Result<Vec<DetectionResult>> {
        // 1. é¢„å¤„ç†ï¼ˆä¸»çº¿ç¨‹ï¼Œæ— RGAå†²çªï¼‰
        let preprocessor = ImagePreprocessor::new();
        let preprocessed = preprocessor.preprocess(img);

        // 2. åˆ›å»ºä»»åŠ¡é€šé“
        let (sender, receiver) = std::sync::mpsc::channel();

        // 3. æäº¤æ¨ç†ä»»åŠ¡
        let task = InferenceTask {
            task_id: 0,
            preprocessed_image: preprocessed,
            original_size: (img.width(), img.height()),
            sender,
        };

        {
            let mut queue = self.task_queue.lock().unwrap();
            // é˜Ÿåˆ—æ»¡äº†å°±ä¸¢å¼ƒæ—§ä»»åŠ¡
            if queue.len() >= 10 {
                queue.remove(0);
                println!("âš ï¸  Queue full, dropping oldest task");
            }
            queue.push(task);
        }

        // 4. ç­‰å¾…ç»“æœ
        match receiver.recv_timeout(Duration::from_secs(5)) {
            Ok(Ok(results)) => Ok(results),
            Ok(Err(e)) => Err(e),
            Err(_) => Err("Timeout".into()),
        }
    }

    /// æ‰¹é‡æ¨ç†
    fn batch_inference(&self, images: &[image::RgbImage]) -> Vec<Result<Vec<DetectionResult>>> {
        let mut results = Vec::new();

        for img in images {
            results.push(self.inference(img));
        }

        results
    }
}

impl Drop for OptimizedInferenceService {
    fn drop(&mut self) {
        println!("ğŸ›‘ Shutting down inference service...");

        self.shutdown.store(true, std::sync::atomic::Ordering::Relaxed);

        for worker in self.workers.drain(..) {
            let _ = worker.join();
        }

        println!("âœ… Inference service shutdown complete");
    }
}

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘      YOLOv8 RKNN ä¼˜åŒ–æ¨ç†æœåŠ¡æµ‹è¯•                â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let model_path = "models/yolov8m.rknn";
    let test_image_path = "tests/test.jpg";
    let num_workers = 6; // å¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼Œå› ä¸ºæ²¡æœ‰RGAå†²çª

    println!("æµ‹è¯•é…ç½®:");
    println!("  æ¨¡å‹è·¯å¾„:     {}", model_path);
    println!("  æµ‹è¯•å›¾ç‰‡:     {}", test_image_path);
    println!("  å·¥ä½œçº¿ç¨‹æ•°:   {}", num_workers);
    println!("  æ¶æ„:         é¢„å¤„ç†+åå¤„ç†åœ¨ä¸»çº¿ç¨‹ï¼Œæ¨ç†åœ¨å¤šçº¿ç¨‹");
    println!();

    // 1. åŠ è½½æµ‹è¯•å›¾ç‰‡
    println!("ğŸ“· åŠ è½½æµ‹è¯•å›¾ç‰‡...");
    let img = image::open(test_image_path).expect("Failed to load test image");
    let img = img.to_rgb8();
    println!("âœ“ å›¾ç‰‡åŠ è½½æˆåŠŸ: {}x{}", img.width(), img.height());
    println!();

    // 2. åˆ›å»ºæ¨ç†æœåŠ¡
    println!("ğŸš€ åˆ›å»ºä¼˜åŒ–æ¨ç†æœåŠ¡...");
    let service = Arc::new(OptimizedInferenceService::new(model_path, num_workers)
        .expect("Failed to create inference service"));
    println!("âœ“ æ¨ç†æœåŠ¡åˆ›å»ºæˆåŠŸ");
    println!();

    // 3. æµ‹è¯• 1: å•ä¸ªæ¨ç†ä»»åŠ¡
    println!("ğŸ§ª æµ‹è¯• 1: å•ä¸ªæ¨ç†ä»»åŠ¡");
    test_single_inference(&service, &img);

    // 4. æµ‹è¯• 2: é¡ºåºæ¨ç†ä»»åŠ¡ï¼ˆ10ä¸ªä»»åŠ¡ï¼‰
    println!("ğŸ§ª æµ‹è¯• 2: é¡ºåºæ¨ç†ä»»åŠ¡ï¼ˆ10ä¸ªä»»åŠ¡ï¼‰");
    test_sequential_inference(&service, &img, 10);

    // 5. æµ‹è¯• 3: å¹¶å‘æ¨ç†ä»»åŠ¡ï¼ˆ4ä¸ªä»»åŠ¡ï¼‰
    println!("ğŸ§ª æµ‹è¯• 3: å¹¶å‘æ¨ç†ä»»åŠ¡ï¼ˆ4ä¸ªä»»åŠ¡ï¼‰");
    test_concurrent_inference(&service, &img, 4);

    // 6. æµ‹è¯• 4: é«˜è´Ÿè½½æµ‹è¯•ï¼ˆ8ä¸ªå¹¶å‘ä»»åŠ¡ï¼‰
    println!("ğŸ§ª æµ‹è¯• 4: é«˜è´Ÿè½½æµ‹è¯•ï¼ˆ8ä¸ªå¹¶å‘ä»»åŠ¡ï¼‰");
    test_high_load(&service, &img, 8);

    println!("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼");
}

fn test_single_inference(service: &Arc<OptimizedInferenceService>, img: &image::RgbImage) {
    println!("  è¿è¡Œå•ä¸ªæ¨ç†ä»»åŠ¡...");

    let start = Instant::now();
    let result = service.inference(img);
    let elapsed = start.elapsed();

    match result {
        Ok(detections) => {
            println!("    âœ“ æ¨ç†æˆåŠŸ: {:.2}ms, æ£€æµ‹åˆ° {} ä¸ªå¯¹è±¡", elapsed.as_millis(), detections.len());
            for (i, det) in detections.iter().take(3).enumerate() {
                println!("      å¯¹è±¡ {}: ç½®ä¿¡åº¦ {:.2}%, ç±»åˆ« {}",
                         i + 1,
                         det.confidence * 100.0,
                         get_class_name(det.class_id).unwrap_or("unknown".to_string()));
            }
        }
        Err(e) => {
            println!("    âœ— æ¨ç†å¤±è´¥: {:?}", e);
        }
    }
    println!();
}

fn test_sequential_inference(service: &Arc<OptimizedInferenceService>, img: &image::RgbImage, num_tasks: usize) {
    println!("  è¿è¡Œ {} ä¸ªé¡ºåºæ¨ç†ä»»åŠ¡...", num_tasks);

    let start_total = Instant::now();
    let mut inference_times = Vec::new();
    let mut success_count = 0;

    for i in 0..num_tasks {
        let start = Instant::now();
        let result = service.inference(img);
        let elapsed = start.elapsed();

        match result {
            Ok(detections) => {
                inference_times.push(elapsed.as_millis());
                success_count += 1;
                println!("    ä»»åŠ¡ {}: {:.2}ms, {} ä¸ªå¯¹è±¡", i + 1, elapsed.as_millis(), detections.len());
            }
            Err(e) => {
                println!("    ä»»åŠ¡ {}: å¤±è´¥ ({:?})", i + 1, e);
            }
        }

        if (i + 1) % 5 == 0 {
            println!("    å®Œæˆ {}/{} ä»»åŠ¡", i + 1, num_tasks);
        }
    }

    let total_time = start_total.elapsed();

    println!("  é¡ºåºæµ‹è¯•ç»“æœ:");
    println!("    æ€»è€—æ—¶: {:.2}ms", total_time.as_millis());
    println!("    æˆåŠŸç‡: {}/{} ({:.1}%)", success_count, num_tasks, success_count as f64 / num_tasks as f64 * 100.0);

    if !inference_times.is_empty() {
        let min_time = inference_times.iter().min().unwrap();
        let max_time = inference_times.iter().max().unwrap();
        let avg_time = inference_times.iter().sum::<u128>() as f64 / inference_times.len() as f64;
        println!("    æ¨ç†æ—¶é—´ç»Ÿè®¡:");
        println!("      æœ€å°: {:.2}ms", min_time);
        println!("      æœ€å¤§: {:.2}ms", max_time);
        println!("      å¹³å‡: {:.2}ms", avg_time);
        println!("      ååé‡: {:.2} FPS", 1000.0 / avg_time);
    }
    println!();
}

fn test_concurrent_inference(service: &Arc<OptimizedInferenceService>, img: &image::RgbImage, num_tasks: usize) {
    println!("  è¿è¡Œ {} ä¸ªå¹¶å‘ä»»åŠ¡...", num_tasks);

    let start_total = Instant::now();
    let mut handles = Vec::new();

    for i in 0..num_tasks {
        let service_clone = service.clone();
        let img_clone = img.clone();

        let handle = thread::spawn(move || {
            let start = Instant::now();
            let result = service_clone.inference(&img_clone);
            let elapsed = start.elapsed();
            (i, result, elapsed)
        });

        handles.push(handle);
    }

    let mut completed_tasks = 0;
    let mut total_detections = 0;
    let mut inference_times = Vec::new();

    for handle in handles {
        match handle.join() {
            Ok((task_id, result, elapsed)) => {
                completed_tasks += 1;
                let elapsed_ms = elapsed.as_millis();

                match result {
                    Ok(detections) => {
                        inference_times.push(elapsed_ms);
                        total_detections += detections.len();
                        println!("    ä»»åŠ¡ {}: {:.2}ms, {} ä¸ªå¯¹è±¡", task_id + 1, elapsed_ms, detections.len());
                    }
                    Err(e) => {
                        println!("    ä»»åŠ¡ {}: å¤±è´¥ ({:?})", task_id + 1, e);
                    }
                }
            }
            Err(e) => {
                println!("    çº¿ç¨‹é”™è¯¯: {:?}", e);
            }
        }
    }

    let total_elapsed = start_total.elapsed();

    println!("  å¹¶å‘æµ‹è¯•ç»“æœ:");
    println!("    å®Œæˆä»»åŠ¡æ•°: {}", completed_tasks);
    println!("    æ€»æ£€æµ‹å¯¹è±¡: {}", total_detections);
    println!("    æ€»è€—æ—¶: {:.2}ms", total_elapsed.as_millis());

    if !inference_times.is_empty() {
        let avg_time = inference_times.iter().sum::<u128>() as f64 / inference_times.len() as f64;
        let theoretical_time = avg_time * num_tasks as f64;
        let efficiency = theoretical_time / total_elapsed.as_millis() as f64;

        println!("    å¹¶å‘æ•ˆç‡:");
        println!("      å¹³å‡æ¨ç†æ—¶é—´: {:.2}ms", avg_time);
        println!("      ç†è®ºæ€»æ—¶é—´: {:.2}ms", theoretical_time);
        println!("      å®é™…æ€»æ—¶é—´: {:.2}ms", total_elapsed.as_millis());
        println!("      å¹¶å‘å€æ•°:   {:.2}x", efficiency);
        println!("      æ•ˆç‡:        {:.1}%", (efficiency / num_tasks as f64 * 100.0));
    }
    println!();
}

fn test_high_load(service: &Arc<OptimizedInferenceService>, img: &image::RgbImage, num_tasks: usize) {
    println!("  é«˜è´Ÿè½½æµ‹è¯•ï¼šå¿«é€Ÿæäº¤ {} ä¸ªå¹¶å‘ä»»åŠ¡...", num_tasks);

    let start_total = Instant::now();
    let mut handles = Vec::new();

    // å¿«é€Ÿæäº¤å¤§é‡ä»»åŠ¡
    for i in 0..num_tasks {
        let service_clone = service.clone();
        let img_clone = img.clone();

        let handle = thread::spawn(move || {
            let start = Instant::now();
            let result = service_clone.inference(&img_clone);
            let elapsed = start.elapsed();
            (i, result, elapsed)
        });

        handles.push(handle);
    }

    let mut completed_tasks = 0;
    let mut total_detections = 0;

    for handle in handles {
        match handle.join() {
            Ok((task_id, result, elapsed)) => {
                completed_tasks += 1;

                match result {
                    Ok(detections) => {
                        total_detections += detections.len();
                        println!("    ä»»åŠ¡ {}: {:.2}ms, {} ä¸ªå¯¹è±¡", task_id + 1, elapsed.as_millis(), detections.len());
                    }
                    Err(e) => {
                        println!("    ä»»åŠ¡ {}: å¤±è´¥ ({:?})", task_id + 1, e);
                    }
                }
            }
            Err(e) => {
                println!("    çº¿ç¨‹é”™è¯¯: {:?}", e);
            }
        }
    }

    let total_elapsed = start_total.elapsed();

    println!("  é«˜è´Ÿè½½æµ‹è¯•ç»“æœ:");
    println!("    æäº¤ä»»åŠ¡æ•°: {}", num_tasks);
    println!("    å®Œæˆä»»åŠ¡æ•°: {}", completed_tasks);
    println!("    æ€»æ£€æµ‹å¯¹è±¡: {}", total_detections);
    println!("    æ€»è€—æ—¶: {:.2}ms", total_elapsed.as_millis());
    println!("    ååé‡: {:.2} FPS", num_tasks as f64 * 1000.0 / total_elapsed.as_millis() as f64);
    println!();
}